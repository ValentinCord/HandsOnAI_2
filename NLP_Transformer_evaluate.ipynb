{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzXk4UKxr+OTlQiAcFGvU2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValentinCord/HandsOnAI_2/blob/main/NLP_Transformer_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span> NLP : Transformer prédiction du modèle </span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "W32Lj_vNV4T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-1\"></a>\n",
        "# <span>1. Installation des packages</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "z62veatQWJ3B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUmsmLoqMmEE",
        "outputId": "87f72732-af2c-4687-ef35-675c53a84971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 28 11:18:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    33W /  70W |   2414MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.25.11)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.8.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n"
          ]
        }
      ],
      "source": [
        "!/opt/bin/nvidia-smi\n",
        "!rm -rf sample_data\n",
        "\n",
        "!pip3 install transformers\n",
        "!pip3 install datasets\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span>2. Imports </span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "_X7WW0uiWMJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basics \n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "\n",
        "# transformers \n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from transformers import CamembertModel, CamembertTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# plot \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "\n",
        "# torch \n",
        "import torch\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# nltk \n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLN9czkDMqc2",
        "outputId": "75eda4ed-2345-4266-b5be-b2b1c83073c5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPGF9arWNPDq",
        "outputId": "7f18c993-669f-4c0f-f80c-4096e374091f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-3\"></a>\n",
        "# <span>3. Choix des paramètres</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "OS1UUEkVWnAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 10\n",
        "VALID_BATCH_SIZE = 10\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "LEN_TEXT = 150\n",
        "OVERLAP = 50\n",
        "\n",
        "TRANSFORMER_NAME = \"cmarkea/distilcamembert-base\"\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/HandOnAI_2_NLP/transformer_model.pth\"\n",
        "test_path = '/content/drive/MyDrive/HandOnAI_2_NLP/fake_test.csv'"
      ],
      "metadata": {
        "id": "vPHx1A4BN65V"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-4\"></a>\n",
        "# <span>4. Lecture des données</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "xj2Z0skKY-A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(test_path)\n",
        "df_test = df_test.drop(['Unnamed: 0', 'target_name'], axis = 1)"
      ],
      "metadata": {
        "id": "5MlXIuCyMztY"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-5\"></a>\n",
        "# <span>5. Preprocessing</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "C-nXfJwEeOBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span>5.1 Nettoyage des données</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "ka_F7DXHZN-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "STOPWORDS = set(stopwords.words('french'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text"
      ],
      "metadata": {
        "id": "bWUTel43NK_f"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['data'] = df_test['data'].apply(clean_text)"
      ],
      "metadata": {
        "id": "NIP0_8R-NLh8"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span>5.2 Découpage des données</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "i4Lhu_BHenzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split(text1):\n",
        "    l_total = []\n",
        "    l_parcial = []\n",
        "    if len(text1.split())//(LEN_TEXT - OVERLAP) >0:\n",
        "        n = len(text1.split())//(LEN_TEXT - OVERLAP)\n",
        "    else: \n",
        "        n = 1\n",
        "    for w in range(n):\n",
        "        if w == 0:\n",
        "            l_parcial = text1.split()[:LEN_TEXT]\n",
        "            l_total.append(\" \".join(l_parcial))\n",
        "        else:\n",
        "            l_parcial = text1.split()[w*(LEN_TEXT - OVERLAP):w*(LEN_TEXT - OVERLAP) + LEN_TEXT]\n",
        "            l_total.append(\" \".join(l_parcial))\n",
        "    return l_total"
      ],
      "metadata": {
        "id": "zXdaL57bNv7E"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['text_split'] = df_test['data'].apply(get_split)\n",
        "df_test['len_split'] = df_test['text_split'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "wWaGn88YNfoT"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span>5.3 Reformulation du labels</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "yRTAMcoXe9YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_df(df): \n",
        "  train_l = []\n",
        "  label_l = []\n",
        "  for idx,row in df.iterrows():\n",
        "      for l in row['text_split']:\n",
        "          train_l.append(l)\n",
        "          label_l.append([1 if row['label'] == i else 0 for i in range(2)])\n",
        "\n",
        "  return pd.DataFrame({'data':train_l, 'label':label_l})"
      ],
      "metadata": {
        "id": "MSKWVc0uNs45"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df_test = create_df(df_test)"
      ],
      "metadata": {
        "id": "6VYcTn_HP7Y1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span>5.4 Création du dataset</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "STcQR_4tfDq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len, is_target = True):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = dataframe\n",
        "        self.text = dataframe.data\n",
        "        self.max_len = max_len\n",
        "        if is_target: \n",
        "          self.targets = self.df.label\n",
        "        else: \n",
        "          self.targets = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        if self.targets is None: \n",
        "          return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(mask, dtype=torch.long),\n",
        "              'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n",
        "          }\n",
        "        else: \n",
        "          return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(mask, dtype=torch.long),\n",
        "              'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "              'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "          }"
      ],
      "metadata": {
        "id": "FpgMTzc9P9HO"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_NAME)\n",
        "testing_set = CustomDataset(cleaned_df_test, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "VouW0PN2STcT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span>5.5 Création du dataloader</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "jWKVB5oLfORn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "-RSpm8A3P-zb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-6\"></a>\n",
        "# <span>6. Chargement du modèle</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "QuPiOF7CfeXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "      super(BERTClass, self).__init__()\n",
        "      self.l1 = CamembertModel.from_pretrained(TRANSFORMER_NAME)\n",
        "      self.l3 = torch.nn.Linear(768, 2) #2 = binary classification\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "      output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "      output = self.l3(output_1['pooler_output'])\n",
        "\n",
        "      return F.softmax(output, dim=1)"
      ],
      "metadata": {
        "id": "Mfq2dgYva22V"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(filepath):\n",
        "    #checkpoint = torch.load(filepath, map_location=torch.device('cpu'))\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = checkpoint['model']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    for parameter in model.parameters():\n",
        "        parameter.requires_grad = False\n",
        "    \n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "_Zk_1QkWSsay"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_checkpoint(model_path)"
      ],
      "metadata": {
        "id": "ChDlJM5ZSyzq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-7\"></a>\n",
        "# <span>7. Prédiction du modèle avec les données splité</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "Hun4UFpWfzB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation():\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    correct_predictions = 0\n",
        "    total_instances = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for count, data in enumerate(testing_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "\n",
        "        # accuracy \n",
        "        classifications = torch.argmax(outputs, dim=1)\n",
        "        labels = torch.argmax(targets, dim=1)\n",
        "        correct_predictions += sum(classifications==labels).item()\n",
        "        total_instances += len(outputs)\n",
        "\n",
        "        # loss \n",
        "        total_loss += ce_loss(outputs, labels)\n",
        "\n",
        "        fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
        "\n",
        "    accuracy = correct_predictions/total_instances\n",
        "    loss = total_loss/total_instances\n",
        "\n",
        "    print(f\"Accuracy Score = {accuracy}\")\n",
        "    print(f\"Loss Score = {loss}\")\n",
        "\n",
        "    return fin_outputs, fin_targets"
      ],
      "metadata": {
        "id": "T0cz9797Snz_"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, targets = validation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLYuyDCATkx0",
        "outputId": "3b259c83-58a7-41ea-8d45-28febe655e3e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score = 0.96\n",
            "Loss Score = 0.03531963378190994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-8\"></a>\n",
        "# <span>8. Prédiction du modèle avec les données d'origine</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "wBeLki1xg55_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df_test['pred'] = outputs\n",
        "pos = 0\n",
        "df_test['pred'] = [list() for x in range(len(df_test.index))]\n",
        "for idx,row in df_test.iterrows():\n",
        "  for i in range(row['len_split']): \n",
        "    row['pred'].append(cleaned_df_test.loc[pos]['pred'])\n",
        "    pos += 1"
      ],
      "metadata": {
        "id": "42l4TxbmVwKu"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['prediction'] = df_test['pred'].apply(lambda x: [1, 0] if np.argmax(np.sum(x, axis = 0)) == 0 else [0, 1])\n",
        "df_test['label_pred'] = df_test['pred'].apply(lambda x: np.argmax(np.sum(x, axis = 0)))"
      ],
      "metadata": {
        "id": "_I61wWbwVy79"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = metrics.accuracy_score(df_test['label_pred'], df_test['label'])\n",
        "print(f\"Accuracy Score = {accuracy}\")"
      ],
      "metadata": {
        "id": "_huKflkvV1V9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381b3a45-e0e6-4db6-b5c6-2daa56d922e3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score = 0.948559670781893\n"
          ]
        }
      ]
    }
  ]
}