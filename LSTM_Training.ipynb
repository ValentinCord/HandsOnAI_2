{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/ValentinCord/HandsOnAI_2/blob/main/NLP_LSTM.ipynb",
      "authorship_tag": "ABX9TyMXsu9wxkGp+5FobDdkPvLG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValentinCord/HandsOnAI_2/blob/main/LSTM_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span> NLP : Entrainnement et sauvegarde du modèle LSTM/GRU </span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "ejL-F7TlidK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [Installations](#section-1)\n",
        "* [Imports](#section-2)\n",
        "* [Choix des paramètres](#section-3)\n",
        "* [Lecture des données](#section-4)\n",
        "* [Preprocessing](#section-5)\n",
        "* [Création du modèle](#section-6)\n",
        "* [Entrainement du modèle](#section-7)\n",
        "* [Prédiction des données](#section-8)\n",
        "* [Sauvegarde du modèle](#section-9)"
      ],
      "metadata": {
        "id": "k-qY5JmTP1RV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-1\"></a>\n",
        "# <span>1. Installation des packages</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "Q5aiJco6irx0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFQtTH6eRi7x",
        "outputId": "f225f04b-5e41-4b48-ca9c-99d38470db41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 27 09:56:26 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!/opt/bin/nvidia-smi\n",
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-2\"></a>\n",
        "# <span>2. Imports </span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "bDjwnN7wiwhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basics \n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
        "\n",
        "# plot \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "\n",
        "# nltk \n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwLPPhiXRnJT",
        "outputId": "394a6c00-6c55-41b7-b7aa-2b31c2970ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHvqXFo7R1lh",
        "outputId": "9347da5b-6443-4fba-96f6-fa0b94eeca9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-3\"></a>\n",
        "# <span>3. Choix des paramètres</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">\n",
        "\n",
        "<p align=\"justify\">Dans cette section, nous pouvons définir les paramètres du modèles LSTM. Comme paramètres d'entrés, on retrouve la taille des séquences, la taille des données d'entrainement et de validations. Il est également possible de définir des paramètres d'entrainements du modèle. Parmis ceux-ci, on retrouve le batch size ainsi que le nombre d'epochs. Le path des données d'entrainement, ajoutées et de tests est aussi défini dans cette section.</p>"
      ],
      "metadata": {
        "id": "jJ8uDoJujvlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 5000\n",
        "DONNEE_AJOUTEES = 200\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "\n",
        "train_path = '/content/drive/MyDrive/HandOnAI_2_NLP/fake_train.csv'\n",
        "added_path = '/content/drive/MyDrive/HandOnAI_2_NLP/added_train.csv'\n",
        "test_path = '/content/drive/MyDrive/HandOnAI_2_NLP/fake_test.csv'"
      ],
      "metadata": {
        "id": "HZ5rkyqBR2e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-4\"></a>\n",
        "# <span>4. Lecture des données</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">\n",
        "<p align=\"justify\">La section suivante consiste simplement à récupérer les donnéeset les stocker dans des dataframes à l'aide de la librairie pandas.</p>"
      ],
      "metadata": {
        "id": "2bzqk1z5j0tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(train_path)\n",
        "df_added = pd.read_csv(added_path)\n",
        "df_test = pd.read_csv(test_path)"
      ],
      "metadata": {
        "id": "7WwgYN8njzYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-5\"></a>\n",
        "# <span>5. Preprocessing</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">\n",
        "<p align=\"justify\">La partie preprocessing est composé de deux parties. Dans la première étape, appliquons un nettoyage de données. Les caractères spéciaux ainsi que les stopwords sont supprimés des données d'entrainnement et de test. Les données abérants trouvées lors de l'analyse de données sont également enlevées. Durant la seconde étape, les données sont tokenisées. </p>"
      ],
      "metadata": {
        "id": "XyUEVv2Aj6MT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span>5.1 Nettoyage de données</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "SXKZ7uaskBuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Unnamed: 0', 'target_name'], axis = 1)\n",
        "df_added.rename(columns = {'french':'data'}, inplace = True)\n",
        "df_added = df_added.drop(['Unnamed: 0'], axis = 1)\n",
        "df_test = df_test.drop(['Unnamed: 0', 'target_name'], axis = 1)\n",
        "\n",
        "df = df.append(df_added[:DONNEE_AJOUTEES], ignore_index=True)"
      ],
      "metadata": {
        "id": "SllvJg2RR_SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "STOPWORDS = set(stopwords.words('french'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text"
      ],
      "metadata": {
        "id": "LS-mVpScSWEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['data'] = df['data'].apply(clean_text)\n",
        "df_test['data'] = df_test['data'].apply(clean_text)\n",
        "\n",
        "df = df.drop(df.index[1430])\n",
        "df = df.drop(df.index[1429])\n",
        "df = df.drop(df.index[1180])\n",
        "df = df.drop(df.index[1136])\n",
        "     \n",
        "df = df.reset_index()"
      ],
      "metadata": {
        "id": "KUrsa_g5SmBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## <span>5.2 Tokenisation des données</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "EPtGK2A8kae-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "\n",
        "X_train = df_train.data.tolist()\n",
        "X_val = df_val.data.tolist()\n",
        "X_test = df_test.data.tolist()\n",
        "\n",
        "y_train = df_train.label.tolist()\n",
        "y_val = df_val.label.tolist()\n",
        "y_test = df_test.label.tolist()\n",
        "\n",
        "train_text_vec = [text for text in X_train]\n",
        "val_text_vec = [text for text in X_val]\n",
        "test_text_vec = [text for text in X_test]\n",
        "\n",
        "# tokenize the sentences\n",
        "tokenizer = Tokenizer(lower=False)\n",
        "tokenizer.fit_on_texts(train_text_vec)\n",
        "\n",
        "train_text_vec = tokenizer.texts_to_sequences(train_text_vec)\n",
        "val_text_vec = tokenizer.texts_to_sequences(val_text_vec)\n",
        "test_text_vec = tokenizer.texts_to_sequences(test_text_vec)\n",
        "\n",
        "# pad the sequences\n",
        "train_text_vec = pad_sequences(train_text_vec, maxlen=MAX_SEQ_LEN)\n",
        "val_text_vec = pad_sequences(val_text_vec, maxlen=MAX_SEQ_LEN)\n",
        "test_text_vec = pad_sequences(test_text_vec, maxlen=MAX_SEQ_LEN)\n"
      ],
      "metadata": {
        "id": "9FYvvznQSoJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot Encode Y values:\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "y_train = encoder.fit_transform(df_train['label'].values)\n",
        "y_train = to_categorical(y_train) \n",
        "\n",
        "y_val = encoder.fit_transform(df_val['label'].values)\n",
        "y_val = to_categorical(y_val) \n",
        "\n",
        "y_test = encoder.fit_transform(df_test['label'].values)\n",
        "y_test = to_categorical(y_test) \n"
      ],
      "metadata": {
        "id": "ENwt9vR-0wrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-6\"></a>\n",
        "# <span>6. Création du modèle</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">\n",
        "<p align=\"justify\">Pour la construction du modèle, nous appliquons en amont une couvche convolutionnelle afin d'extraire des caractéristiques. Suite à l'extraction de caractériques, nous appliquons plusieurs couches LSTM. Ces couches sont bidirectionnelles afin d'avoir un maximum d'informations. Le modèle se termine avec de simples couches afin d'effectuer la classification. </p>"
      ],
      "metadata": {
        "id": "eiXuZZ6lkXPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
        "model.add(Conv1D(32,3, padding='same'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "metadata": {
        "id": "ML7pAvvY01Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-7\"></a>\n",
        "# <span>7. Entrainement du modèle</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">\n",
        "<p align=\"justify\">Maintenant que l'architecture est terminée, l'entrainement peut commencer. Comme loss nous avons choisit le crossentropy binaire avec l'optimizer adam. </p>"
      ],
      "metadata": {
        "id": "9NCRQXA4kq39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_text_vec, y_train, validation_data=(val_text_vec, y_val), batch_size=batch_size, epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuJW1Rx-sIvr",
        "outputId": "187eacc0-c475-4d95-8a4e-16055392e735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "21/21 [==============================] - 26s 586ms/step - loss: 0.6326 - accuracy: 0.6183 - val_loss: 0.3875 - val_accuracy: 0.8489\n",
            "Epoch 2/3\n",
            "21/21 [==============================] - 11s 503ms/step - loss: 0.2524 - accuracy: 0.9176 - val_loss: 0.3299 - val_accuracy: 0.8761\n",
            "Epoch 3/3\n",
            "21/21 [==============================] - 11s 504ms/step - loss: 0.0464 - accuracy: 0.9894 - val_loss: 0.3566 - val_accuracy: 0.9215\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5bc43720d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-8\"></a>\n",
        "# <span>8. Évaluation du modèle</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "rGb2L7l_cklz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_scores = model.evaluate(test_text_vec, y_test, verbose=1)\n",
        "print(\"test scores:\", test_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RwyocdX9hmq",
        "outputId": "27d54e2f-847c-48dd-f353-c8d185c6fd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 146ms/step - loss: 0.2086 - accuracy: 0.9568\n",
            "test scores: [0.20857766270637512, 0.9567901492118835]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-9\"></a>\n",
        "# <span>9. Sauvegarde du modèle et du tokenizer</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "nt5jNPLsowBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/HandOnAI_2_NLP/LSTM_model.h5')"
      ],
      "metadata": {
        "id": "aeFSns3UoFzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/HandOnAI_2_NLP/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "7gVONVsRAOPU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}