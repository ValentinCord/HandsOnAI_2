{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWYgROcGcraOMdMjXCLAda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValentinCord/HandsOnAI_2/blob/main/NLP_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span> NLP : Entrainnement et sauvegarde du modèle LSTM/GRU </span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "ejL-F7TlidK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-1\"></a>\n",
        "# <span>1. Installation des packages</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "Q5aiJco6irx0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFQtTH6eRi7x",
        "outputId": "2802da60-b0b8-49f9-fac7-9f1b54e641ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 23 20:12:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    33W /  70W |   4986MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.8.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n"
          ]
        }
      ],
      "source": [
        "!/opt/bin/nvidia-smi\n",
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span>2. Imports </span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "bDjwnN7wiwhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basics \n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
        "\n",
        "# plot \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "\n",
        "# nltk \n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwLPPhiXRnJT",
        "outputId": "6ac2b9cc-caf9-4b96-cb7e-a8dbd921b4c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHvqXFo7R1lh",
        "outputId": "3c66514e-fe4e-4352-e19c-7552a5efd46c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-3\"></a>\n",
        "# <span>3. Choix des paramètres</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "jJ8uDoJujvlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 5000\n",
        "DONNEE_AJOUTEES = 200\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "\n",
        "train_path = '/content/drive/MyDrive/HandOnAI_2_NLP/fake_train.csv'\n",
        "added_path = '/content/drive/MyDrive/HandOnAI_2_NLP/added_train.csv'\n",
        "test_path = '/content/drive/MyDrive/HandOnAI_2_NLP/fake_test.csv'"
      ],
      "metadata": {
        "id": "HZ5rkyqBR2e4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-4\"></a>\n",
        "# <span>4. Lecture des données</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "2bzqk1z5j0tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(train_path)\n",
        "df_added = pd.read_csv(added_path)\n",
        "df_test = pd.read_csv(test_path)"
      ],
      "metadata": {
        "id": "7WwgYN8njzYH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-5\"></a>\n",
        "# <span>5. Preprocessing</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "XyUEVv2Aj6MT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <span>5.1 Nettoyage de données</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "SXKZ7uaskBuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Unnamed: 0', 'target_name'], axis = 1)\n",
        "df_added.rename(columns = {'french':'data'}, inplace = True)\n",
        "df_added = df_added.drop(['Unnamed: 0'], axis = 1)\n",
        "df_test = df_test.drop(['Unnamed: 0', 'target_name'], axis = 1)\n",
        "\n",
        "df = df.append(df_added[:DONNEE_AJOUTEES], ignore_index=True)"
      ],
      "metadata": {
        "id": "SllvJg2RR_SP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "STOPWORDS = set(stopwords.words('french'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text"
      ],
      "metadata": {
        "id": "LS-mVpScSWEh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['data'] = df['data'].apply(clean_text)\n",
        "df_test['data'] = df_test['data'].apply(clean_text)\n",
        "\n",
        "df = df.drop(df.index[1136])\n",
        "df = df.reset_index()"
      ],
      "metadata": {
        "id": "KUrsa_g5SmBz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-6\"></a>\n",
        "# <span>6. Tokenisation des données</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "EPtGK2A8kae-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "\n",
        "X_train = df_train.data.tolist()\n",
        "X_val = df_val.data.tolist()\n",
        "X_test = df_test.data.tolist()\n",
        "\n",
        "y_train = df_train.label.tolist()\n",
        "y_val = df_val.label.tolist()\n",
        "y_test = df_test.label.tolist()\n",
        "\n",
        "train_text_vec = [text for text in X_train]\n",
        "val_text_vec = [text for text in X_val]\n",
        "test_text_vec = [text for text in X_test]\n",
        "\n",
        "# tokenize the sentences\n",
        "tokenizer = Tokenizer(lower=False)\n",
        "tokenizer.fit_on_texts(train_text_vec)\n",
        "\n",
        "train_text_vec = tokenizer.texts_to_sequences(train_text_vec)\n",
        "val_text_vec = tokenizer.texts_to_sequences(val_text_vec)\n",
        "test_text_vec = tokenizer.texts_to_sequences(test_text_vec)\n",
        "\n",
        "# pad the sequences\n",
        "train_text_vec = pad_sequences(train_text_vec, maxlen=MAX_SEQ_LEN)\n",
        "val_text_vec = pad_sequences(val_text_vec, maxlen=MAX_SEQ_LEN)\n",
        "test_text_vec = pad_sequences(test_text_vec, maxlen=MAX_SEQ_LEN)\n"
      ],
      "metadata": {
        "id": "9FYvvznQSoJo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot Encode Y values:\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "y_train = encoder.fit_transform(df_train['label'].values)\n",
        "y_train = to_categorical(y_train) \n",
        "\n",
        "y_val = encoder.fit_transform(df_val['label'].values)\n",
        "y_val = to_categorical(y_val) \n",
        "\n",
        "y_test = encoder.fit_transform(df_test['label'].values)\n",
        "y_test = to_categorical(y_test) \n"
      ],
      "metadata": {
        "id": "ENwt9vR-0wrv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-7\"></a>\n",
        "# <span>7. Création du modèle</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "eiXuZZ6lkXPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
        "model.add(Conv1D(32,3, padding='same'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "metadata": {
        "id": "ML7pAvvY01Un"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-8\"></a>\n",
        "# <span>8. Entrainement du modèle</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "9NCRQXA4kq39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_text_vec, y_train, validation_data=(val_text_vec, y_val), batch_size=batch_size, epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuJW1Rx-sIvr",
        "outputId": "a307bf74-77a9-48e0-951f-b27a64d84b8b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "21/21 [==============================] - 5s 167ms/step - loss: 0.6870 - accuracy: 0.5411 - val_loss: 0.5555 - val_accuracy: 0.7801\n",
            "Epoch 2/3\n",
            "21/21 [==============================] - 3s 146ms/step - loss: 0.4061 - accuracy: 0.8619 - val_loss: 0.2682 - val_accuracy: 0.9157\n",
            "Epoch 3/3\n",
            "21/21 [==============================] - 3s 144ms/step - loss: 0.1058 - accuracy: 0.9758 - val_loss: 0.2310 - val_accuracy: 0.9217\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8da6a3f790>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_scores = model.evaluate(test_text_vec, y_test, verbose=1)\n",
        "\n",
        "print(\"test scores:\", test_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RwyocdX9hmq",
        "outputId": "e4a114d2-22ba-44cd-df80-d0ef736ebee9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 1s 57ms/step - loss: 0.1703 - accuracy: 0.9300\n",
            "test scores: [0.17031842470169067, 0.9300411343574524]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-9\"></a>\n",
        "# <span>9. Sauvegarde du modèle</span>\n",
        "<hr style=\"border-bottom: solid;background-color:light;color:black;\">"
      ],
      "metadata": {
        "id": "nt5jNPLsowBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/HandOnAI_2_NLP/LSTM_model.h5')"
      ],
      "metadata": {
        "id": "aeFSns3UoFzt"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}